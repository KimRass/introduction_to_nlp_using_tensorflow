{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/KimRass/Programming/blob/master/Data%20Science/Machine%20Learning/NLP/fra-eng%20%26%20Character-Level%20seq2seq%20(NMT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NlLx6Y8jgkDU",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Training\" data-toc-modified-id=\"Training-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Training</a></span></li><li><span><a href=\"#Inference\" data-toc-modified-id=\"Inference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Inference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5PcG7sU2Wwq",
    "outputId": "2ca9d9fd-addf-4dbe-8ee9-acf60386dcec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount(\"/content/drive\")\n",
    "os.chdir(\"/content/drive/MyDrive/Libraries\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.layers import Layer, Dense, Flatten, Dropout, Concatenate, Add, Dot, Multiply, Reshape, Activation, BatchNormalization, SimpleRNNCell, RNN, SimpleRNN, LSTM, Embedding, Bidirectional, TimeDistributed, Conv1D, Conv2D, MaxPool1D, MaxPool2D, GlobalMaxPool1D, GlobalMaxPool2D, AveragePooling1D, AveragePooling2D, GlobalAveragePooling1D, GlobalAveragePooling2D, ZeroPadding2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adagrad\n",
    "from tensorflow.keras.metrics import MeanSquaredError, RootMeanSquaredError, MeanAbsoluteError, MeanAbsolutePercentageError, BinaryCrossentropy, CategoricalCrossentropy, SparseCategoricalCrossentropy, CosineSimilarity\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.activations import linear, sigmoid, relu\n",
    "from tensorflow.keras.initializers import RandomNormal, glorot_uniform, he_uniform, Constant\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import zipfile\n",
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T02:11:34.754058Z",
     "start_time": "2022-01-27T02:11:33.945189Z"
    },
    "id": "w650GKn9YTN8"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/content/drive/MyDrive/NLP\")\n",
    "raw_data = pd.read_table(\"./Datasets/fra-eng/fra.txt\", usecols=[0, 1], names=[\"tar\", \"src\"])\n",
    "\n",
    "raw_data = raw_data.sample(len(raw_data)//3, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T02:11:37.624378Z",
     "start_time": "2022-01-27T02:11:34.757018Z"
    },
    "id": "Pr959qoTgkDe"
   },
   "outputs": [],
   "source": [
    "# `lower`: Whether to convert the texts to lowercase.\n",
    "# `char_level`: If `True`, every character will be treated as a token.\n",
    "tokenizer_src = Tokenizer(char_level=True)\n",
    "tokenizer_src.fit_on_texts(raw_data[\"src\"])\n",
    "char2idx_src = tokenizer_src.word_index\n",
    "vocab_size_src = len(char2idx_src)\n",
    "enc_input = tokenizer_src.texts_to_sequences(raw_data[\"src\"])\n",
    "\n",
    "tokenizer_tar = Tokenizer(char_level=True)\n",
    "tokenizer_tar.fit_on_texts(\"시\" + raw_data[\"tar\"] + \"종\")\n",
    "char2idx_tar = tokenizer_tar.word_index\n",
    "vocab_size_tar = len(char2idx_tar)\n",
    "dec_input = tokenizer_tar.texts_to_sequences(\"시\" + raw_data[\"tar\"])\n",
    "dec_gt = tokenizer_tar.texts_to_sequences(raw_data[\"tar\"] + \"종\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T02:11:37.680230Z",
     "start_time": "2022-01-27T02:11:37.626346Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spk0qLNOa0jE",
    "outputId": "9a1a84f7-00eb-411e-b3e8-1f4e9ad11626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "길이가 가장 긴 문장의 길이는 305이고 길이가 86 이하인 문장이 전체의 99%를 차지합니다.\n",
      "길이가 가장 긴 문장의 길이는 240이고 길이가 72 이하인 문장이 전체의 99%를 차지합니다.\n"
     ]
    }
   ],
   "source": [
    "ratio = 0.99\n",
    "\n",
    "lens_enc = sorted([len(doc) for doc in enc_input])\n",
    "max_len_enc = int(np.quantile(lens_enc, ratio))\n",
    "print(f\"길이가 가장 긴 문장의 길이는 {np.max(lens_enc)}이고 길이가 {max_len_enc} 이하인 문장이 전체의 {ratio:.0%}를 차지합니다.\")\n",
    "\n",
    "lens_dec = sorted([len(doc) for doc in dec_input])\n",
    "max_len_dec = int(np.quantile(lens_dec, ratio))\n",
    "print(f\"길이가 가장 긴 문장의 길이는 {np.max(lens_dec)}이고 길이가 {max_len_dec} 이하인 문장이 전체의 {ratio:.0%}를 차지합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T02:12:14.516699Z",
     "start_time": "2022-01-27T02:11:37.682202Z"
    },
    "id": "mPi-I91BYTOB"
   },
   "outputs": [],
   "source": [
    "enc_input = pad_sequences(enc_input, padding=\"post\", maxlen=max_len_enc)\n",
    "dec_input = pad_sequences(dec_input, padding=\"post\", maxlen=max_len_dec)\n",
    "dec_gt = pad_sequences(dec_gt, padding=\"post\", maxlen=max_len_dec)\n",
    "\n",
    "enc_input = to_categorical(enc_input)\n",
    "dec_input = to_categorical(dec_input)\n",
    "dec_gt = to_categorical(dec_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDF2hXV9gkDg"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-27T02:12:29.523570Z",
     "start_time": "2022-01-27T02:12:15.440230Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMrzRNokgkDh",
    "outputId": "5a678de8-0b28-452f-8966-2d3e8183d3d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input_enc (InputLayer)         [(None, 86, 76)]     0           []                               \n",
      "                                                                                                  \n",
      " Input_dec (InputLayer)         [(None, 72, 61)]     0           []                               \n",
      "                                                                                                  \n",
      " LSTM_enc (LSTM)                [(None, 256),        340992      ['Input_enc[0][0]']              \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " LSTM_dec (LSTM)                [(None, 72, 256),    325632      ['Input_dec[0][0]',              \n",
      "                                 (None, 256),                     'LSTM_enc[0][1]',               \n",
      "                                 (None, 256)]                     'LSTM_enc[0][2]']               \n",
      "                                                                                                  \n",
      " Dense_dec (Dense)              (None, 72, 61)       15677       ['LSTM_dec[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 682,301\n",
      "Trainable params: 682,301\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 2.5251 - acc: 0.5503\n",
      "Epoch 00001: val_acc improved from -inf to 0.57605, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 238s 11s/step - loss: 2.5251 - acc: 0.5503 - val_loss: 1.7893 - val_acc: 0.5761\n",
      "Epoch 2/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.6310 - acc: 0.5945 \n",
      "Epoch 00002: val_acc improved from 0.57605 to 0.60428, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 239s 11s/step - loss: 1.6310 - acc: 0.5945 - val_loss: 1.4943 - val_acc: 0.6043\n",
      "Epoch 3/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.4211 - acc: 0.6172 \n",
      "Epoch 00003: val_acc improved from 0.60428 to 0.61994, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 258s 12s/step - loss: 1.4211 - acc: 0.6172 - val_loss: 1.3705 - val_acc: 0.6199\n",
      "Epoch 4/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3481 - acc: 0.6269 \n",
      "Epoch 00004: val_acc improved from 0.61994 to 0.63602, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 248s 12s/step - loss: 1.3481 - acc: 0.6269 - val_loss: 1.3244 - val_acc: 0.6360\n",
      "Epoch 5/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.3080 - acc: 0.6358 \n",
      "Epoch 00005: val_acc improved from 0.63602 to 0.63826, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 256s 12s/step - loss: 1.3080 - acc: 0.6358 - val_loss: 1.2906 - val_acc: 0.6383\n",
      "Epoch 6/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2778 - acc: 0.6410 \n",
      "Epoch 00006: val_acc improved from 0.63826 to 0.64705, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 259s 12s/step - loss: 1.2778 - acc: 0.6410 - val_loss: 1.2631 - val_acc: 0.6470\n",
      "Epoch 7/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2497 - acc: 0.6490 \n",
      "Epoch 00007: val_acc improved from 0.64705 to 0.65140, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 236s 11s/step - loss: 1.2497 - acc: 0.6490 - val_loss: 1.2347 - val_acc: 0.6514\n",
      "Epoch 8/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.2207 - acc: 0.6562 \n",
      "Epoch 00008: val_acc improved from 0.65140 to 0.65922, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 235s 11s/step - loss: 1.2207 - acc: 0.6562 - val_loss: 1.2040 - val_acc: 0.6592\n",
      "Epoch 9/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1876 - acc: 0.6664 \n",
      "Epoch 00009: val_acc improved from 0.65922 to 0.67222, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 238s 11s/step - loss: 1.1876 - acc: 0.6664 - val_loss: 1.1790 - val_acc: 0.6722\n",
      "Epoch 10/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1531 - acc: 0.6792 \n",
      "Epoch 00010: val_acc improved from 0.67222 to 0.68551, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 249s 12s/step - loss: 1.1531 - acc: 0.6792 - val_loss: 1.1302 - val_acc: 0.6855\n",
      "Epoch 11/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.1098 - acc: 0.6940 \n",
      "Epoch 00011: val_acc improved from 0.68551 to 0.70032, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 254s 12s/step - loss: 1.1098 - acc: 0.6940 - val_loss: 1.0870 - val_acc: 0.7003\n",
      "Epoch 12/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0677 - acc: 0.7035 \n",
      "Epoch 00012: val_acc improved from 0.70032 to 0.70682, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 258s 12s/step - loss: 1.0677 - acc: 0.7035 - val_loss: 1.0489 - val_acc: 0.7068\n",
      "Epoch 13/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0326 - acc: 0.7077 \n",
      "Epoch 00013: val_acc improved from 0.70682 to 0.70886, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 251s 12s/step - loss: 1.0326 - acc: 0.7077 - val_loss: 1.0174 - val_acc: 0.7089\n",
      "Epoch 14/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.0055 - acc: 0.7103 \n",
      "Epoch 00014: val_acc improved from 0.70886 to 0.71131, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 249s 12s/step - loss: 1.0055 - acc: 0.7103 - val_loss: 0.9933 - val_acc: 0.7113\n",
      "Epoch 15/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9834 - acc: 0.7135 \n",
      "Epoch 00015: val_acc improved from 0.71131 to 0.71555, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 255s 12s/step - loss: 0.9834 - acc: 0.7135 - val_loss: 0.9744 - val_acc: 0.7155\n",
      "Epoch 16/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9720 - acc: 0.7162 \n",
      "Epoch 00016: val_acc improved from 0.71555 to 0.71718, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 250s 12s/step - loss: 0.9720 - acc: 0.7162 - val_loss: 0.9611 - val_acc: 0.7172\n",
      "Epoch 17/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9533 - acc: 0.7209 \n",
      "Epoch 00017: val_acc improved from 0.71718 to 0.72277, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 249s 12s/step - loss: 0.9533 - acc: 0.7209 - val_loss: 0.9464 - val_acc: 0.7228\n",
      "Epoch 18/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9400 - acc: 0.7244 \n",
      "Epoch 00018: val_acc improved from 0.72277 to 0.72607, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 251s 12s/step - loss: 0.9400 - acc: 0.7244 - val_loss: 0.9342 - val_acc: 0.7261\n",
      "Epoch 19/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9307 - acc: 0.7268 \n",
      "Epoch 00019: val_acc improved from 0.72607 to 0.72847, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 252s 12s/step - loss: 0.9307 - acc: 0.7268 - val_loss: 0.9255 - val_acc: 0.7285\n",
      "Epoch 20/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9195 - acc: 0.7294 \n",
      "Epoch 00020: val_acc improved from 0.72847 to 0.72872, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 253s 12s/step - loss: 0.9195 - acc: 0.7294 - val_loss: 0.9163 - val_acc: 0.7287\n",
      "Epoch 21/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9104 - acc: 0.7313 \n",
      "Epoch 00021: val_acc improved from 0.72872 to 0.73122, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 244s 12s/step - loss: 0.9104 - acc: 0.7313 - val_loss: 0.9055 - val_acc: 0.7312\n",
      "Epoch 22/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.9031 - acc: 0.7323 \n",
      "Epoch 00022: val_acc improved from 0.73122 to 0.73570, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 248s 12s/step - loss: 0.9031 - acc: 0.7323 - val_loss: 0.8955 - val_acc: 0.7357\n",
      "Epoch 23/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8932 - acc: 0.7354 \n",
      "Epoch 00023: val_acc improved from 0.73570 to 0.73643, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 238s 11s/step - loss: 0.8932 - acc: 0.7354 - val_loss: 0.8889 - val_acc: 0.7364\n",
      "Epoch 24/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8837 - acc: 0.7378 \n",
      "Epoch 00024: val_acc improved from 0.73643 to 0.73802, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 237s 11s/step - loss: 0.8837 - acc: 0.7378 - val_loss: 0.8796 - val_acc: 0.7380\n",
      "Epoch 25/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8740 - acc: 0.7406 \n",
      "Epoch 00025: val_acc improved from 0.73802 to 0.74079, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 244s 12s/step - loss: 0.8740 - acc: 0.7406 - val_loss: 0.8707 - val_acc: 0.7408\n",
      "Epoch 26/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8672 - acc: 0.7423 \n",
      "Epoch 00026: val_acc improved from 0.74079 to 0.74304, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 255s 12s/step - loss: 0.8672 - acc: 0.7423 - val_loss: 0.8648 - val_acc: 0.7430\n",
      "Epoch 27/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8622 - acc: 0.7437 \n",
      "Epoch 00027: val_acc improved from 0.74304 to 0.74423, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 256s 12s/step - loss: 0.8622 - acc: 0.7437 - val_loss: 0.8590 - val_acc: 0.7442\n",
      "Epoch 28/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8555 - acc: 0.7454 \n",
      "Epoch 00028: val_acc improved from 0.74423 to 0.74530, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 248s 12s/step - loss: 0.8555 - acc: 0.7454 - val_loss: 0.8531 - val_acc: 0.7453\n",
      "Epoch 29/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8498 - acc: 0.7470 \n",
      "Epoch 00029: val_acc improved from 0.74530 to 0.74728, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 241s 12s/step - loss: 0.8498 - acc: 0.7470 - val_loss: 0.8475 - val_acc: 0.7473\n",
      "Epoch 30/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8443 - acc: 0.7482 \n",
      "Epoch 00030: val_acc improved from 0.74728 to 0.74856, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 240s 11s/step - loss: 0.8443 - acc: 0.7482 - val_loss: 0.8422 - val_acc: 0.7486\n",
      "Epoch 31/32\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.8388 - acc: 0.7497 \n",
      "Epoch 00031: val_acc improved from 0.74856 to 0.75009, saving model to ./fra_eng_char-level_seq2seq.h5\n",
      "21/21 [==============================] - 238s 11s/step - loss: 0.8388 - acc: 0.7497 - val_loss: 0.8374 - val_acc: 0.7501\n",
      "Epoch 32/32\n"
     ]
    }
   ],
   "source": [
    "name = \"./fra_eng_char-level_seq2seq\"\n",
    "model_path = f\"{name}.h5\"\n",
    "hist_path = f\"{name}_hist.npy\"\n",
    "if os.path.exists(model_path):\n",
    "    model = load_model(model_path)\n",
    "    hist = np.load(hist_path, allow_pickle=\"TRUE\").item()\n",
    "else:\n",
    "    inputs_enc = Input(shape=(max_len_enc, vocab_size_src + 1), name=\"Input_enc\")\n",
    "    inputs_dec = Input(shape=(max_len_dec, vocab_size_tar + 1), name=\"Input_dec\")\n",
    "    \n",
    "    _, h_state, c_state = LSTM(units=256, return_state=True, name=\"LSTM_enc\")(inputs_enc)\n",
    "    z, _, _ = LSTM(units=256, return_sequences=True, return_state=True, name=\"LSTM_dec\")(inputs_dec, initial_state=[h_state, c_state])\n",
    "    outputs = Dense(units=vocab_size_tar + 1, activation=\"softmax\", name=\"Dense_dec\")(z)\n",
    "\n",
    "    model = Model(inputs=[inputs_enc, inputs_dec], outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor=\"val_loss\", mode=\"auto\", verbose=1, patience=1)\n",
    "    mc = ModelCheckpoint(filepath=model_path, monitor=\"val_acc\", mode=\"auto\", verbose=1, save_best_only=True)\n",
    "    hist = model.fit(x=[enc_input, dec_input], y=dec_gt, batch_size=2048, epochs=32, validation_split=0.3, callbacks=[es, mc])\n",
    "    \n",
    "    np.save(hist_path, hitst.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQqqHIhRYTOD",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습이 맞게 됐는지 확인\n",
    "i = 110\n",
    "pred = model.predict([tf.expand_dims(enc_input[i], axis=0), tf.expand_dims(dec_input[i], axis=0)])\n",
    "\n",
    "sent = \"\"\n",
    "for idx in tf.argmax(dec_gt[i], axis=1).numpy():\n",
    "    if idx != 0:\n",
    "        sent += idx2char_tar[idx]\n",
    "print(sent)\n",
    "\n",
    "sent = \"\"\n",
    "for idx in tf.argmax(pred[0], axis=1).numpy():\n",
    "    if idx != 0:\n",
    "        sent += idx2char_tar[idx]\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQDLJkvCYTOE"
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rD5vl9rcYTOF"
   },
   "source": [
    "- 우선 인코더를 정의합니다. enc_inputs와 encoder_states는 훈련 과정에서 이미 정의한 것들을 재사용하는 것입니다. 이제 디코더를 설계해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "54KFid58fJoC",
    "outputId": "3a4c805b-effe-4f00-a98b-94e1a91603f1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f97529d7438>,\n",
       " <tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f9752d01f60>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f97529d7908>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x7f9753533278>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f9753533cc0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "NGBHG4gTYTOF",
    "outputId": "e729e08f-5fd7-4d20-8568-4c2de1753ae1",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputs_enc = model.layers[0].output\n",
    "_, h_state, c_state = model.layers[2].output\n",
    "\n",
    "enc_model = tf.keras.Model(inputs=inputs_enc, outputs=[h_state, c_state])\n",
    "\n",
    "inputs_dec = model.layers[1].output\n",
    "h_state_bef = Input(shape=(256,))\n",
    "c_state_bef = Input(shape=(256,))\n",
    "# 문장의 다음 단어를 예측하기 위해서 initial_state를 이전 시점의 상태로 사용합니다.\n",
    "lstm_dec_layer = model.layers[3]\n",
    "lstm_dec, h_state_aft, c_state_aft = lstm_dec_layer(inputs_dec, initial_state=[h_state_bef, c_state_bef])\n",
    "dense_dec_layer = model.layers[4]\n",
    "dense_dec = dense_dec_layer(lstm_dec)\n",
    "\n",
    "dec_model = tf.keras.Model(inputs=[inputs_dec]+[h_state_bef, c_state_bef], outputs=[dense_dec]+[h_state_aft, c_state_aft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "MqbVBmExYTOG",
    "outputId": "9be7b630-3b2b-429e-a87a-7af26720a027"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def decode_seq(input_seq):\n",
    "# seq = enc_input[i:i+1]\n",
    "    enc_states = enc_model.predict(input_seq)\n",
    "\n",
    "    # <SOS>에 해당하는 OHE를 생성합니다.\n",
    "    seq = np.zeros((1, 1, len(char2idx_tar)+1))\n",
    "    seq[0, 0, char2idx_tar[\"<SOS>\"]] = 1\n",
    "\n",
    "    stop_cond = False\n",
    "    decoded_sent = \"\"\n",
    "    # stop_cond이 True가 될 때까지 반복합니다.\n",
    "    while not stop_cond:\n",
    "        # 이점 시점의 states를 현재 시점의 states로 사용합니다.\n",
    "        output_tokens, h_state, c_state = dec_model.predict([seq] + enc_states)\n",
    "        argmax = np.argmax(output_tokens[0, -1, :])\n",
    "    #     argmax = np.argmax(output_tokens[0, 0])\n",
    "        char = idx2char_tar[argmax]\n",
    "        decoded_sent += char\n",
    "        # 현재 시점의 예측 결과를 다음 시점의 입력으로 사용하기 위해 저장합니다.\n",
    "        seq = np.zeros((1, 1, len(char2idx_tar)+1))\n",
    "        seq[0, 0, argmax] = 1\n",
    "        enc_states = [h_state, c_state]\n",
    "        \n",
    "        # \"<EOS>\"에 도달하거나 최대 길이를 넘으면 stop_cond=True를 저장합니다.\n",
    "        if char == \"<EOS>\" or len(decoded_sent) == max_len_dec:\n",
    "            stop_cond = True\n",
    "    return decoded_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "ZbbUbHkjzpuk",
    "outputId": "d73b7b61-0b3a-46af-a650-beb4337ad829"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <style>\n",
       "    pre {white-space: pre-wrap;}\n",
       "  </style>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "입력 문장 : why do you need change?\n",
      "정답 문장 : ourquoi as-tu besoin de changement \n",
      "번역 문장 : jz3lw(++1yj1…w+l1+jl…zp3+lœ1ljz… <EOS\n",
      "BLEU-1 : 0\n",
      "BLEU-2 : 0\n",
      "BLEU-3 : 0\n",
      "BLEU-4 : 0\n",
      "-----------------------------------\n",
      "입력 문장 : why did you change your mind?\n",
      "정답 문장 : ourquoi as-tu changé d'avis \n",
      "번역 문장 : j(l+z0êp+lœ;(s1êjl3;(lw(+l8j8ljê(s(p//8 <EOS\n",
      "BLEU-1 : 0\n",
      "BLEU-2 : 0\n",
      "BLEU-3 : 0\n",
      "BLEU-4 : 0\n",
      "-----------------------------------\n",
      "입력 문장 : we don't want to lose you.\n",
      "정답 문장 : ous ne voulons pas vous perdre\n",
      "번역 문장 : sz0+l(s1)ljz0jlœ1l+0pj1lùl/(l…(p+z3 <EOS\n",
      "BLEU-1 : 0\n",
      "BLEU-2 : 0\n",
      "BLEU-3 : 0\n",
      "BLEU-4 : 0\n",
      "-----------------------------------\n",
      "입력 문장 : see that this never happens again.\n",
      "정답 문장 : aites en sorte que ça ne se produise plus\n",
      "번역 문장 : èj1+ysz0+lwêèj1lùlc(pê1lh(l&<EOS\n",
      "BLEU-1 : 0\n",
      "BLEU-2 : 0\n",
      "BLEU-3 : 0\n",
      "BLEU-4 : 0\n",
      "-----------------------------------\n",
      "입력 문장 : you must not lose sight of your main object.\n",
      "정답 문장 : l ne faut pas que tu perdes de vue ton objectif principal\n",
      "번역 문장 : /1+lwêp4l+z3jl+0êl/1lwzp3jlœ1lw(ê/1êlœ1l/(l3z0êêpj0ê1l1jlœ1l/;(êç13j <EOS\n",
      "BLEU-1 : 0\n",
      "BLEU-2 : 0\n",
      "BLEU-3 : 0\n",
      "BLEU-4 : 0\n"
     ]
    }
   ],
   "source": [
    "actual, pred = list(), list()\n",
    "for seq_index in range(231, 236):\n",
    "    input_seq = enc_input[seq_index:seq_index+1]\n",
    "    decoded_sent = decode_seq(input_seq)\n",
    "    \n",
    "    actual.append([data[\"tar\"][seq_index][1:len(data[\"tar\"][seq_index])-1].split()])\n",
    "    pred.append(decoded_sent[:len(decoded_sent)-1].split())\n",
    "                  \n",
    "    print(35 * \"-\")\n",
    "    print(f\"입력 문장 : {data['src'][seq_index]}\")\n",
    "    print(f\"정답 문장 : {data['tar'][seq_index][1:len(data['tar'][seq_index])-1]}\")\n",
    "    print(f\"번역 문장 : {decoded_sent[:len(decoded_sent)-1]}\")\n",
    "    sf = SmoothingFunction()\n",
    "    print(f\"BLEU-1 : {corpus_bleu(actual, pred, weights=(1, 0, 0, 0),\\\n",
    "                                  smoothing_function=sf.method1)}\")\n",
    "    print(f\"BLEU-2 : {corpus_bleu(actual, pred, weights=(1/2, 1/2, 0, 0),\\\n",
    "                                  smoothing_function=sf.method1)}\")\n",
    "    print(f\"BLEU-3 : {corpus_bleu(actual, pred, weights=(1/3, 1/3, 1/3, 0),\\\n",
    "                                  smoothing_function=sf.method1)}\")\n",
    "    print(f\"BLEU-4 : {corpus_bleu(actual, pred, weights=(1/4, 1/4, 1/4, 1/4),\\\n",
    "                                  smoothing_function=sf.method1)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "fra-eng & Character-Level seq2seq (NMT).ipynb",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "191.594px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
